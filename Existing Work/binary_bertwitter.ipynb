{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4a-fn49FVB7"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers==3.4.0\n",
        "!pip3 install emoji\n",
        "!pip3 install keras\n",
        "!pip3 install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G26Ut1fFfx1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaModel\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertModel\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn import metrics\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "from transformers import AdamW\n",
        "from sklearn.utils import class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdBRQUP6Ff1P"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "#torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g7ouC3RFf4o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6696, 93)\n"
          ]
        }
      ],
      "source": [
        "# linguistic features\n",
        "df = pd.read_csv('./LIWC2015_Results_1.csv', header=None)\n",
        "features = np.array(df.loc[:, 1:])\n",
        "print(features.shape) # (6696, 93)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTmVyjTQFf7h"
      },
      "outputs": [],
      "source": [
        "# Set the maximum sequence length\n",
        "MAX_LEN = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiAepfZiFf-E"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../bragging_data/bragging_data.csv', header=0, names=['id', 'text', 'sampling', 'round_no', 'label'])\n",
        "\n",
        "text_training = []\n",
        "text_testing = []\n",
        "label_training = []\n",
        "label_testing = []\n",
        "labels_number = []\n",
        "index_training = []\n",
        "index_testing = []\n",
        "features_training = []\n",
        "features_testing = []\n",
        "\n",
        "text_testing_print = []\n",
        "\n",
        "texts = df.text.values\n",
        "sampling_ways = df.sampling.values\n",
        "round_nos = df.round_no.values\n",
        "labels = df.label.values\n",
        "\n",
        "for i in labels:\n",
        "    if i == \"not\":\n",
        "        labels_number.append(0)\n",
        "    else:\n",
        "        labels_number.append(1)\n",
        "\n",
        "for i in range(len(sampling_ways)):\n",
        "    if sampling_ways[i] == 'keyword':\n",
        "        index_training.append(i)\n",
        "        #text_training.append(texts[i])\n",
        "        label_training.append(labels_number[i])\n",
        "        features_training.append(features[i])\n",
        "    else:\n",
        "        index_testing.append(i)\n",
        "        #text_testing.append(texts[i])\n",
        "        text_testing_print.append(texts[i])\n",
        "        label_testing.append(labels_number[i])\n",
        "        features_testing.append(features[i]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC0YPJD2FoLG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10975  7454   104 ...     0     0     0]\n",
            " [10975  7454   104 ...     0     0     0]\n",
            " [10975  7454   104 ...     0     0     0]\n",
            " ...\n",
            " [10975  7454   104 ...     0     0     0]\n",
            " [10975  7454   104 ... 38022     8 29804]\n",
            " [10975  7454   104 ...     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "texts = ['[CLS] ' + str(sentence) + ' [SEP]' for sentence in texts]\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in texts]\n",
        "\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "# Pad input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n",
        "print(input_ids)\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8thlzjlFqy6"
      },
      "outputs": [],
      "source": [
        "mask_training = []\n",
        "mask_testing = []\n",
        "for i in index_training:\n",
        "    text_training.append(input_ids[i])\n",
        "    mask_training.append(attention_masks[i])\n",
        "for i in index_testing:\n",
        "    text_testing.append(input_ids[i])\n",
        "    mask_testing.append(attention_masks[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0gPpLOfxFrhv"
      },
      "outputs": [],
      "source": [
        "hidden_size = 768\n",
        "embedding_size = 400\n",
        "beta = 0.001\n",
        "dropout_prob = 0.5\n",
        "\n",
        "class AttnGating(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AttnGating, self).__init__()\n",
        "   \n",
        "    self.linear = nn.Linear(93, embedding_size)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.weight_emotion_W1 = nn.Parameter(torch.Tensor(hidden_size+embedding_size, hidden_size))\n",
        "    self.weight_emotion_W2 = nn.Parameter(torch.Tensor(embedding_size, hidden_size))\n",
        " \n",
        "    \n",
        "    nn.init.uniform_(self.weight_emotion_W1, -0.1, 0.1)\n",
        "    nn.init.uniform_(self.weight_emotion_W2, -0.1, 0.1)\n",
        "\n",
        "    self.LayerNorm = nn.LayerNorm(hidden_size)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "  def forward(self, embeddings_roberta, linguistic_feature):\n",
        "     \n",
        "     # Project linguistic representations into vectors with comparable size\n",
        "     linguistic_feature = self.linear(linguistic_feature)\n",
        "     emotion_feature = linguistic_feature.repeat(MAX_LEN, 1, 1) # (50, bs, 200) \n",
        "     emotion_feature = emotion_feature.permute(1, 0, 2) # (bs, 50, 200)\n",
        "\n",
        "     # Concatnate word and linguistic representations  \n",
        "     features_combine = torch.cat((emotion_feature, embeddings_roberta), axis=2) # (bs, 50, 968)\n",
        "     \n",
        "     g_feature = self.relu(torch.matmul(features_combine, self.weight_emotion_W1))\n",
        "\n",
        "     # Attention gating\n",
        "     H = torch.mul(g_feature, torch.matmul(emotion_feature, self.weight_emotion_W2))\n",
        "     alfa = min(beta * (torch.norm(embeddings_roberta)/torch.norm(H)), 1)\n",
        "     E = torch.add(torch.mul(alfa, H), embeddings_roberta)\n",
        "\n",
        "     # Layer normalization and dropout \n",
        "     embedding_output = self.dropout(self.LayerNorm(E)) \n",
        "\n",
        "     return E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "p3mJNsQXFs1T"
      },
      "outputs": [],
      "source": [
        "class RobertaClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClassificationModel, self).__init__()\n",
        "\n",
        "        self.embedding_roberta = AutoModel.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True)\n",
        "        #self.attn_gate = AttnGating()\n",
        "\n",
        "        self.roberta = AutoModel.from_pretrained(\"vinai/bertweet-base\", return_dict=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        #self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.num_labels_bragging = 2\n",
        "      \n",
        "        self.classifier_bragging = nn.Linear(768, 2)\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, input_feature, attention_mask, labels=None):\n",
        "\n",
        "        embedding_roberta = self.embedding_roberta(input_ids)\n",
        "        #last_hidden_state, pooler_output, all_hidden_states = self.embedding_roberta(input_ids)\n",
        "        last_hidden_state = embedding_roberta['last_hidden_state']\n",
        "        pooler_output = embedding_roberta['pooler_output']\n",
        "        all_hidden_states = embedding_roberta['hidden_states']\n",
        "        roberta_embed = all_hidden_states[0]\n",
        "        #combine_embed = self.attn_gate(roberta_embed, input_feature)\n",
        "\n",
        "        outputs = self.roberta(input_ids=None, inputs_embeds=roberta_embed)\n",
        "        sequence_output  = outputs.last_hidden_state\n",
        "\n",
        "        x = sequence_output[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits_bragging = self.classifier_bragging(x)\n",
        "\n",
        "\n",
        "        # Initialize loss of bragging \n",
        "        loss_bragging = None \n",
        "\n",
        "        # Training on bragging\n",
        "        if labels is not None:\n",
        "          if self.num_labels_bragging == 1:\n",
        "            loss_fct_bragging = nn.MSELoss()\n",
        "            loss_bragging = loss_fct_bragging(logits_bragging.view(-1), labels.view(-1))\n",
        "          else:\n",
        "            loss_fct_bragging = nn.CrossEntropyLoss()\n",
        "            loss_bragging = loss_fct_bragging(logits_bragging.view(-1, self.num_labels_bragging), labels.view(-1))\n",
        "\n",
        "\n",
        "        output = (logits_bragging,) +outputs[2:]\n",
        "\n",
        "      \n",
        "        return ((loss_bragging,) + output) if loss_bragging is not None else output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2YaB3oXgFvkq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3382\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "630f3db61aa74f5d87b1558aeaf57901",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/var/folders/n1/xx3q3j_j73lgcm303jlydw6r0000gr/T/ipykernel_24298/736250599.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
            "  x_train = torch.LongTensor(x_train)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "RobertaForSequenceClassification.forward() got an unexpected keyword argument 'input_feature'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [25], line 78\u001b[0m\n\u001b[1;32m     73\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     75\u001b[0m \u001b[39m# Generate combined representations      \u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[39m=\u001b[39m model_roberta(input_ids\u001b[39m=\u001b[39;49mb_input_ids, input_feature\u001b[39m=\u001b[39;49mb_input_features, attention_mask\u001b[39m=\u001b[39;49mb_input_mask, labels\u001b[39m=\u001b[39;49mb_labels)\n\u001b[1;32m     79\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     80\u001b[0m logits \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[0;31mTypeError\u001b[0m: RobertaForSequenceClassification.forward() got an unexpected keyword argument 'input_feature'"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "n_epoch = 12\n",
        "\n",
        "x_train = text_training\n",
        "y_train = label_training\n",
        "\n",
        "\n",
        "print(len(x_train))\n",
        "x_dev, x_testing, y_dev, y_testing, features_dev, features_testing, mask_dev, mask_testing = train_test_split(text_testing, label_testing, features_testing, mask_testing, test_size=0.8, random_state=0, stratify=label_testing)\n",
        "\n",
        "\n",
        "# sklearn\n",
        "weight_loss = class_weight.compute_class_weight( class_weight ='balanced', classes =  np.unique(y_train), y = y_train)\n",
        "weight_loss = torch.tensor(weight_loss, dtype=torch.float32).cpu()\n",
        "\n",
        "#model_roberta = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).cuda()\n",
        "#model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2).cuda()\n",
        "model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2).cpu()\n",
        "\n",
        "#model_roberta = RobertaClassificationModel().cpu()\n",
        "#model_roberta.cuda()\n",
        "\n",
        "param_optimizer = list(model_roberta.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.weight']        \n",
        "    \n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model_roberta.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in model_roberta.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-6)\n",
        "criterion = nn.CrossEntropyLoss(weight=weight_loss, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
        "\n",
        "x_train = torch.LongTensor(x_train)\n",
        "x_dev = torch.LongTensor(x_dev)\n",
        "\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_dev = torch.LongTensor(y_dev)\n",
        "\n",
        "features_train = torch.FloatTensor(features_training)\n",
        "features_dev = torch.FloatTensor(features_dev)\n",
        "\n",
        "mask_train = torch.LongTensor(mask_training)\n",
        "mask_dev = torch.LongTensor(mask_dev)\n",
        "\n",
        "# Pack to dataLoader\n",
        "train_data = TensorDataset(x_train, features_train, mask_train, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    \n",
        "dev_data = TensorDataset(x_dev, features_dev, mask_dev, y_dev)\n",
        "dev_sampler = RandomSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size) \n",
        "\n",
        "# Initialize previous dev loss\n",
        "previous_valid_loss = 1000\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    print(epoch)\n",
        "\n",
        "    # Training\n",
        "    model_roberta.train()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from dataloader\n",
        "        b_input_ids, b_input_features, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Generate combined representations      \n",
        "\n",
        "        \n",
        "        outputs = model_roberta(input_ids=b_input_ids, input_feature=b_input_features, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        #loss = criterion(logits, b_labels)\n",
        "\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # track train loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        \n",
        "    train_loss = np.average(train_losses)\n",
        "    print('train loss: {}'.format(train_loss))\n",
        "\n",
        "    # Validation\n",
        "    model_roberta.eval()\n",
        "\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in dev_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from dataloader\n",
        "        b_input_ids, b_input_features, b_input_mask, b_labels = batch\n",
        "        \n",
        "      \n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Generate combined representations\n",
        "            \n",
        "            outputs = model_roberta(input_ids=b_input_ids, input_feature=b_input_features, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "            #loss = criterion(logits, b_labels)\n",
        "          \n",
        "\n",
        "        valid_losses.append(loss.item())\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        \n",
        "        labels = b_labels.to('cpu').numpy() \n",
        "\n",
        "        predictions = np.append(predictions, np.argmax(logits, axis=1))\n",
        "        targets = np.append(targets, labels) \n",
        "\n",
        "    # Calculate total dev loss \n",
        "    valid_loss = np.average(valid_losses)\n",
        "    print('valid loss: {}'.format(valid_loss))\n",
        "\n",
        "    # Calculate dev f1 of sverity\n",
        "    dev_f1 = metrics.f1_score(targets, predictions, average='macro', zero_division=1)\n",
        "    print(\"dev_f1:\", dev_f1)\n",
        "\n",
        "\n",
        "    # Save the best model based on dev lossr\n",
        "    torch.save(model_roberta, './bragging.pkl')\n",
        "    print(\"saved\")             \n",
        "  \n",
        "x_testing = torch.LongTensor(x_testing)\n",
        "y_testing = torch.LongTensor(y_testing)\n",
        "features_testing = torch.FloatTensor(features_testing)\n",
        "mask_testing = torch.LongTensor(mask_testing)\n",
        "    \n",
        "test_data = TensorDataset(x_testing, features_testing, mask_testing, y_testing) \n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Testing\n",
        "bragging_model = torch.load('./bragging.pkl')\n",
        "\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "bragging_model.eval()\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from dataloader\n",
        "    b_input_ids, b_input_features, b_input_mask, b_labels = batch\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = bragging_model(input_ids=b_input_ids, input_feature=b_input_features, attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "    test_predictions = np.append(test_predictions, np.argmax(logits, axis=1))\n",
        "    test_targets = np.append(test_targets, labels)\n",
        "\n",
        "test_acc = metrics.accuracy_score(test_targets, test_predictions)\n",
        "test_precision = metrics.precision_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n",
        "test_recall = metrics.recall_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n",
        "test_f1 = metrics.f1_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n",
        "print(\"test_acc:\", test_acc)\n",
        "print(\"test_precision:\", test_precision)\n",
        "print(\"test_recall:\", test_recall)\n",
        "print(\"test_f1:\", test_f1)\n",
        "\n",
        "target_names = ['class 0', 'class 1']\n",
        "print(metrics.classification_report(test_targets, test_predictions, target_names=target_names))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "binary_bertwitter_liwc.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
